<!DOCTYPE html>
<html>
<head>
    <title>DeepFlavor Emotion Demo</title>
    <link rel="stylesheet" type="text/css" href="styles.css">
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.4.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@0.0.1/dist/face-landmarks-detection.js"></script>
</head>
<body>
    <div id="content">
        <div id="webcam-container">
            <video id="webcam" playsinline autoplay></video>
            <canvas id="output"></canvas>
        </div>
        <div id="side-container">
            <div id="logo-container">
                <img src="https://i.imgur.com/XIExD09.png" alt="Logo 1" />
            </div>
            <div id="emotions-container">
                <h1 id="status">Loading...</h1>
                <div id="emotions"></div>
            </div>
        </div>
    </div>
    <script>
        function setText(text) {
            document.getElementById("status").innerText = text;
        }

        function drawLine(ctx, x1, y1, x2, y2) {
            ctx.beginPath();
            ctx.moveTo(x1, y1);
            ctx.lineTo(x2, y2);
            ctx.stroke();
        }

        function displayEmotionPercentages(percentages) {
            const emotionsDiv = document.getElementById("emotions");
            emotionsDiv.innerHTML = "";
            for (const emotion in percentages) {
                const line = document.createElement("p");
                line.innerText = `${emotion}: ${(percentages[emotion] * 100).toFixed(2)}%`;
                emotionsDiv.appendChild(line);
            }
        }

        async function setupWebcam() {
            return new Promise((resolve, reject) => {
                const webcamElement = document.getElementById("webcam");
                const navigatorAny = navigator;
                navigator.getUserMedia = navigator.getUserMedia ||
                    navigatorAny.webkitGetUserMedia || navigatorAny.mozGetUserMedia ||
                    navigatorAny.msGetUserMedia;
                if (navigator.getUserMedia) {
                    navigator.getUserMedia({ video: true },
                        stream => {
                            webcamElement.srcObject = stream;
                            webcamElement.addEventListener("loadeddata", resolve, false);
                        },
                        error => reject());
                } else {
                    reject();
                }
            });
        }

        const emotions = ["angry", "disgust", "fear", "happy", "neutral", "sad", "surprise"];
        let emotionModel = null;

        let output = null;
        let model = null;

        async function predictEmotion(points) {
            let result = tf.tidy(() => {
                const xs = tf.stack([tf.tensor1d(points)]);
                return emotionModel.predict(xs);
            });
            let prediction = await result.data();
            result.dispose();
            let emotionPercentages = {};
            for (let i = 0; i < emotions.length; i++) {
                emotionPercentages[emotions[i]] = prediction[i];
            }
            return emotionPercentages;
        }

        async function trackFace() {
            const video = document.querySelector("video");
            const faces = await model.estimateFaces({
                input: video,
                returnTensors: false,
                flipHorizontal: false,
            });
            output.drawImage(
                video,
                0, 0, video.width, video.height,
                0, 0, video.width, video.height
            );

            let points = null;
            faces.forEach(face => {
                const x1 = face.boundingBox.topLeft[0];
                const y1 = face.boundingBox.topLeft[1];
                const x2 = face.boundingBox.bottomRight[0];
                const y2 = face.boundingBox.bottomRight[1];
                const bWidth = x2 - x1;
                const bHeight = y2 - y1;
                drawLine(output, x1, y1, x2, y1);
                drawLine(output, x2, y1, x2, y2);
                drawLine(output, x1, y2, x2, y2);
                drawLine(output, x1, y1, x1, y2);

                const features = [
                    "noseTip",
                    "leftCheek",
                    "rightCheek",
                    "leftEyeLower1", "leftEyeUpper1",
                    "rightEyeLower1", "rightEyeUpper1",
                    "leftEyebrowLower",
                    "rightEyebrowLower",
                    "lipsLowerInner",
                    "lipsUpperInner",
                ];
                points = [];
                features.forEach(feature => {
                    face.annotations[feature].forEach(x => {
                        points.push((x[0] - x1) / bWidth);
                        points.push((x[1] - y1) / bHeight);
                    });
                });
            });

            if (points) {
                let emotionPercentages = await predictEmotion(points);
                let dominantEmotion = Object.keys(emotionPercentages).reduce((a, b) => (emotionPercentages[a] > emotionPercentages[b] ? a : b));
                setText(`Detected: ${dominantEmotion}`);
                displayEmotionPercentages(emotionPercentages);
            } else {
                setText("No Face");
            }

            requestAnimationFrame(trackFace);

        }

        (async () => {
            await setupWebcam();
            const video = document.getElementById("webcam");
            video.play();
            let videoWidth = video.videoWidth;
            let videoHeight = video.videoHeight;
            video.width = videoWidth;
            video.height = videoHeight;

            let canvas = document.getElementById("output");
            canvas.width = video.width;
            canvas.height = video.height;

            output = canvas.getContext("2d");
            output.translate(canvas.width, 0);
            output.scale(-1, 1);
            output.fillStyle = "#fdffb6";
            output.strokeStyle = "#fdffb6";
            output.lineWidth = 2;

            model = await faceLandmarksDetection.load(
                faceLandmarksDetection.SupportedPackages.mediapipeFacemesh
            );

            emotionModel = await tf.loadLayersModel('web/model/facemo.json');

            setText("Loaded!");

            trackFace();
        })();
    </script>
</body>
</html>
