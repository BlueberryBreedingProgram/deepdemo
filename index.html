<!DOCTYPE html>
<html>
<head>
    <title>DeepFlavor Emotion Demo</title>
    <link rel="stylesheet" type="text/css" href="styles.css">
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.4.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@0.0.1/dist/face-landmarks-detection.js"></script>
</head>
<body>
    <div id="content">
        <div id="webcam-container">
            <video id="webcam" playsinline autoplay></video>
            <canvas id="output"></canvas>
        </div>
        <div id="side-container">
            <div id="logo-container">
                <img src="https://i.imgur.com/XIExD09.png" alt="Logo 1" />
            </div>
            <div id="interaction-container">
                <h1 id="status">Grab a Sample and ...</h1>
                <button id="start-button" style="display: none;" onclick="startExperience()">Get Started</button>
                <div id="countdown" style="display:none;">5</div>
                <div id="message" style="display:none;">Taste your berries!</div>
                <div id="emotions-container" style="display:none;">
                    <div id="emotions"></div>
                </div>
            </div>
        </div>
    </div>
    <script>
        function setText(text) {
            document.getElementById("status").innerText = text;
        }

        function drawLine(ctx, x1, y1, x2, y2) {
            ctx.beginPath();
            ctx.moveTo(x1, y1);
            ctx.lineTo(x2, y2);
            ctx.stroke();
        }

        function displayEmotionPercentages(percentages) {
            const emotionsDiv = document.getElementById("emotions");
            emotionsDiv.innerHTML = "";
            for (const emotion in percentages) {
                const line = document.createElement("p");
                line.innerText = `${emotion}: ${(percentages[emotion] * 100).toFixed(2)}%`;
                emotionsDiv.appendChild(line);
            }
        }

        async function setupWebcam() {
            return new Promise((resolve, reject) => {
                const webcamElement = document.getElementById("webcam");
                const navigatorAny = navigator;
                navigator.getUserMedia = navigator.getUserMedia ||
                    navigatorAny.webkitGetUserMedia || navigatorAny.mozGetUserMedia ||
                    navigatorAny.msGetUserMedia;
                if (navigator.getUserMedia) {
                    navigator.getUserMedia({ video: true },
                        stream => {
                            webcamElement.srcObject = stream;
                            webcamElement.addEventListener("loadeddata", resolve, false);
                        },
                        error => reject());
                } else {
                    reject();
                }
            });
        }

        const emotions = ["angry", "disgust", "fear", "happy", "neutral", "sad", "surprise"];
        let emotionModel = null;
        let output = null;
        let model = null;
        let emotionData = [];
        let trackingInterval = null;

        async function predictEmotion(points) {
            let result = tf.tidy(() => {
                const xs = tf.stack([tf.tensor1d(points)]);
                return emotionModel.predict(xs);
            });
            let prediction = await result.data();
            result.dispose();
            let emotionPercentages = {};
            for (let i = 0; i < emotions.length; i++) {
                emotionPercentages[emotions[i]] = prediction[i];
            }
            return emotionPercentages;
        }

        async function trackFace() {
            const video = document.querySelector("video");
            const faces = await model.estimateFaces({
                input: video,
                returnTensors: false,
                flipHorizontal: false,
            });
            output.drawImage(
                video,
                0, 0, video.width, video.height,
                0, 0, video.width, video.height
            );

            let points = null;
            faces.forEach(face => {
                const x1 = face.boundingBox.topLeft[0];
                const y1 = face.boundingBox.topLeft[1];
                const x2 = face.boundingBox.bottomRight[0];
                const y2 = face.boundingBox.bottomRight[1];
                const bWidth = x2 - x1;
                const bHeight = y2 - y1;
                drawLine(output, x1, y1, x2, y1);
                drawLine(output, x2, y1, x2, y2);
                drawLine(output, x1, y2, x2, y2);
                drawLine(output, x1, y1, x1, y2);

                const features = [
                    "noseTip",
                    "leftCheek",
                    "rightCheek",
                    "leftEyeLower1", "leftEyeUpper1",
                    "rightEyeLower1", "rightEyeUpper1",
                    "leftEyebrowLower",
                    "rightEyebrowLower",
                    "lipsLowerInner",
                    "lipsUpperInner",
                ];
                points = [];
                features.forEach(feature => {
                    face.annotations[feature].forEach(x => {
                        points.push((x[0] - x1) / bWidth);
                        points.push((x[1] - y1) / bHeight);
                    });
                });
            });

            if (points) {
                let emotionPercentages = await predictEmotion(points);
                emotionData.push(emotionPercentages);
            }
        }

        function startExperience() {
            document.getElementById("start-button").style.display = "none";
            document.getElementById("countdown").style.display = "block";
            document.getElementById("message").style.display = "block";
            let countdown = 6;
            setText("");
            emotionData = [];
            trackingInterval = setInterval(trackFace, 100);

            const countdownInterval = setInterval(() => {
                document.getElementById("countdown").innerText = countdown;
                if (countdown === 0) {
                    clearInterval(countdownInterval);
                    captureEmotions();
                } else {
                    countdown--;
                }
            }, 1000);
        }

        function captureEmotions() {
            clearInterval(trackingInterval);  // Stop tracking to process emotions
            const averagedEmotions = averageEmotions(emotionData);
            const dominantEmotion = Object.keys(averagedEmotions).reduce((a, b) => (averagedEmotions[a] > averagedEmotions[b] ? a : b));
            setText(`We detected ${dominantEmotion}!`);
            document.getElementById("countdown").style.display = "none";
            document.getElementById("message").style.display = "none";
            document.getElementById("emotions-container").style.display = "block";
            displayEmotionPercentages(averagedEmotions);
            setTimeout(resetView, 6000);
        }

        function averageEmotions(data) {
            const averaged = {};
            data.forEach(emotions => {
                for (const emotion in emotions) {
                    if (!averaged[emotion]) averaged[emotion] = 0;
                    averaged[emotion] += emotions[emotion];
                }
            });
            for (const emotion in averaged) {
                averaged[emotion] /= data.length;
            }
            return averaged;
        }

        function resetView() {
            document.getElementById("status").innerText = "Grab a Sample and ...";
            document.getElementById("start-button").style.display = "block";
            document.getElementById("countdown").style.display = "none";
            document.getElementById("message").style.display = "none";
            document.getElementById("emotions-container").style.display = "none";
            startTracking();  // Restart tracking to ensure webcam is active
        }

        async function startTracking() {
            trackingInterval = setInterval(trackFace, 100);
        }

        (async () => {
            await setupWebcam();
            const video = document.getElementById("webcam");
            video.play();
            let videoWidth = video.videoWidth;
            let videoHeight = video.videoHeight;
            video.width = videoWidth;
            video.height = videoHeight;

            let canvas = document.getElementById("output");
            canvas.width = video.width;
            canvas.height = video.height;

            output = canvas.getContext("2d");
            output.translate(canvas.width, 0);
            output.scale(-1, 1);
            output.fillStyle = "#fdffb6";
            output.strokeStyle = "#fdffb6";
            output.lineWidth = 2;

            model = await faceLandmarksDetection.load(
                faceLandmarksDetection.SupportedPackages.mediapipeFacemesh
            );

            emotionModel = await tf.loadLayersModel('web/model/facemo.json');

            document.getElementById("start-button").style.display = "block"; // Show the start button once everything is loaded
            startTracking();  // Start tracking initially
        })();
    </script>
</body>
</html>




